{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi welcome to the course on text analytics. this is the day-1 of this course'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hi welcome to the Course on TEXT Analytics. \\\n",
    "This is the day-1 of this course\"\n",
    "text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token - Smallest unit of text data -> word, group of words, sentences, paragraphs...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'welcome', 'to', 'the', 'Course', 'on', 'TEXT', 'Analytics.', 'This', 'is', 'the', 'day-1', 'of', 'this', 'course']\n"
     ]
    }
   ],
   "source": [
    "print(text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'hotel', 'is', 'awesome,', \"isn't\", 'it?', 'it', \"couldn't\", 'have', 'been', 'better', 'place', 'than', 'this']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"This hotel is awesome, isn't it? \\\n",
    "it couldn't have been better place than this\"\n",
    "print(text1.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK offers different tokenizers:\n",
    "    1. word_tokenize\n",
    "    2. tweettokenizer\n",
    "    3. regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'hotel', 'is', 'awesome', ',', 'is', \"n't\", 'it', '?', 'it', 'could', \"n't\", 'have', 'been', 'better', 'place', 'than', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'hotel', 'is', 'awesome', 'isnt', 'it', 'it', 'couldnt', 'have', 'been', 'better', 'place', 'than', 'this']\n"
     ]
    }
   ],
   "source": [
    "text1 = re.sub(\"[^\\w\\s]+\",\"\",text1)\n",
    "print(word_tokenize(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'good', 'hot', ',', 'el', ',', 'i', 'li.ked', 'it', 'very', 'muc', '!', 'h', 'did', \"n't\", 'i', '?']\n"
     ]
    }
   ],
   "source": [
    "text2 = \"this is a good hot,el, i li.ked it very muc!h didn't i?\"\n",
    "print(word_tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'good', 'hotel', 'i', 'liked', 'it', 'very', 'much', 'didnt', 'i']\n"
     ]
    }
   ],
   "source": [
    "text2 = re.sub(\"[^\\w\\s]+\",\"\",text2)\n",
    "print(word_tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"LAMO i am not able to use the internet #Stopusing @ITTEAM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LAMO', 'i', 'am', 'not', 'able', 'to', 'use', 'the', 'internet', '#', 'Stopusing', '@', 'ITTEAM']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tok = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LAMO', 'i', 'am', 'not', 'able', 'to', 'use', 'the', 'internet', '#Stopusing', '@ITTEAM']\n"
     ]
    }
   ],
   "source": [
    "print(tweet_tok.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"I am #killing it, luv mah lyf YOLO LOL :D :D <3 @raju\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', '#', 'killing', 'it', ',', 'luv', 'mah', 'lyf', 'YOLO', 'LOL', ':', 'D', ':', 'D', '<', '3', '@', 'raju']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', '#killing', 'it', ',', 'luv', 'mah', 'lyf', 'YOLO', 'LOL', ':D', ':D', '<3', '@raju']\n"
     ]
    }
   ],
   "source": [
    "print(tweet_tok.tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Living life king size #chilling #lifegoals #yolo #wanderlust\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#tokenize the message by extracting only the hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#chilling', '#lifegoals', '#yolo', '#wanderlust']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(message,'#[\\w]+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of tokenizers which we have learnt so far:\n",
    "    \n",
    "    1. Word_tokenze differentiates punctuations(,?!.) with apostrophe\n",
    "    2. Tweet tokenize is specifically used while dealing with social media \n",
    "    text consisting of #, @ and emoticons\n",
    "    3. regexp_tokenize is used when we are interested in few specific \n",
    "    words which follows a common pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_nltk = stopwords.words(\"english\")\n",
    "print(stop_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "print(list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"I am not able to work today, I will be taking Off\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able', 'work', 'today', 'taking']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ word for word in word_tokenize(txt.lower()) \\\n",
    "if word not in list(punctuation) and word not in stop_nltk ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "   - A Technique which takes the word to its root form\n",
    "   - Its rule based and removes suffixes from the words\n",
    "   - The stemmed word might not be part of the dictionary\n",
    "   - 2 types of stemmers:\n",
    "       1. porter stemmer - old stemmer developed in 1979\n",
    "       2. snowball stemmer - supports multiple languages, faster and performs better than porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_p = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drive'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer_p.stem('driving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1 = 'I mustered all my drive, drove to the driving school'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'muster', 'all', 'my', 'drive', ',', 'drove', 'to', 'the', 'drive', 'school']\n",
      "Wall time: 991 µs\n"
     ]
    }
   ],
   "source": [
    "print([stemmer_p.stem(word) for word in word_tokenize(txt1.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_s = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'muster', 'all', 'my', 'drive', ',', 'drove', 'to', 'the', 'drive', 'school']\n",
      "Wall time: 991 µs\n"
     ]
    }
   ],
   "source": [
    "print([stemmer_s.stem(word) for word in word_tokenize(txt1.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stem the below sentences\n",
    "text1 = \"studies studying cries cry his execute\"\n",
    "text2 = \"studies studying cries cry his \\\n",
    "execute orderly university universal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['studi', 'studi', 'cri', 'cri', 'hi', 'execut']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"studies studying cries cry his execute\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['studi', 'studi', 'cri', 'cri', 'hi', 'execut']\n",
      "Wall time: 495 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print([stemmer_p.stem(word) for word in word_tokenize(text1.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['studi', 'studi', 'cri', 'cri', 'his', 'execut']\n",
      "Wall time: 494 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print([stemmer_s.stem(word) for word in word_tokenize(text1.lower())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "- Like stemming, lemmatization takes the word to the root form called as lemma\n",
    "- It involves resolving words to their dictionary form\n",
    "- A lemma of a word is its dictionary form or canonical form\n",
    "- Lemmatizer in python uses a built in dictionary called \"wordnet\" which comrpises of a list of words and their meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"He is very methodical and orderly in his execution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'is', 'very', 'methodical', 'and', 'orderly', 'in', 'his', 'execution']\n"
     ]
    }
   ],
   "source": [
    "print([lemm.lemmatize(word) for word in word_tokenize(text.lower())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizer removes suffixes on nouns by default provided the word after suffix removal is part of the dictionary\n",
    "Lemmatizer does not remove any suffixes if the resultant word cannot be found in the wordnet dictionary\n",
    "Lemmatizer is very aggresive is preserving in meaning of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmetize the below sentenses\n",
    "txt1 = \"he is very methodical and orderly in his execution\"\n",
    "txt2 = \"he is driving and drives the down of the drived vehicle\"\n",
    "txt3 = \"studies studying cries cry likes his execute\"\n",
    "txt4 = \"studies studying cries cry his likes execute \\\n",
    "orderly university universal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'be', 'drive', 'and', 'drive', 'the', 'down', 'of', 'the', 'drive', 'vehicle']\n"
     ]
    }
   ],
   "source": [
    "print([lemm.lemmatize(word,pos=\"v\")\n",
    "       for word in word_tokenize(txt2.lower()) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('\\.',' ',\"this.is.\").strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
